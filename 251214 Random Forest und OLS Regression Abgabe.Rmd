
---
title: "Random Forest mit SHAP-Analyse - Bereinigt"
output: html_notebook
---
Laden der benÃ¶tigten Packages:
```{r setup}
# ---- PACKAGE-SETUP ----
# Alle benÃ¶tigten Bibliotheken laden
suppressPackageStartupMessages({
  library(haven)          # SPSS-Daten laden
  library(tidyverse)      # Datenmanipulation und ggplot2
  library(randomForest)   # Random Forest Modell
  library(caret)          # Machine Learning Framework
  library(corrplot)       # Korrelationsmatrizen
  library(ranger)         # Schnellere Random Forest Implementation
  library(Metrics)        # ZusÃ¤tzliche Metriken
  library(report)         # FÃ¼r die Quellenangaben am Ende
  
})

cat("ðŸ“¦ Alle Pakete erfolgreich geladen!\n\n")
```

Laden der Daten:
```{r data_loading}
# ---- DATEN LADEN UND ERSTE EXPLORATION ----
# SPSS-Datei laden
data_raw <- read_sav("~/Masterthesis/Datensatz Innenstadt/231121_Merged_CityFixed_MergeFixed.sav")

cat("ðŸ“Š Datensatz-Ãœberblick:\n")
cat(paste("Dimensionen:", nrow(data_raw), "Zeilen x", ncol(data_raw), "Spalten\n"))

# Zielvariable definieren
target_var <- "q5_10"  # Anpassen an Ihre Zielvariable

cat("\nðŸŽ¯ Zielvariable Analyse:\n")
print(table(data_raw[[target_var]], useNA = "ifany"))
cat("Bereich:", min(data_raw[[target_var]], na.rm=TRUE), "bis", 
    max(data_raw[[target_var]], na.rm=TRUE), "\n")
```
Entfernen von Zeilen, bei denen die Zielvariable fehlt:
```{r data_cleaning}
# ---- DATENBEREINIGUNG UND -AUFBEREITUNG ----
data_clean <- data_raw %>%
  # Entferne Zeilen mit fehlender Zielvariable
  filter(!is.na(.data[[target_var]])) %>%
  # Konvertiere SPSS Labels zu numerischen Werten
  mutate(across(where(is.labelled) & !where(is.character), ~ as.numeric(.x)))

cat("\nðŸ§¹ Nach Bereinigung:", nrow(data_clean), "Zeilen verbleibend\n")

```
Definition und ÃœberprÃ¼fung der betrachteten PrÃ¤diktorvariablen:
```{r basic_feature_selection}
# ---- BASIS FEATURE-AUSWAHL ----
# Definiere ALLE gewÃ¼nschten Features (ohne Zielvariable)
include_vars <- c(
  # Ã–kologische Faktoren:
  "a1l1_333", "a1l2_334", "a1l3_335", "a1l4_336", "a1l5_337",
  
  # Soziale Faktoren:
  "a2l1_338", "a2l2_339", "a2l3_340", "a2l4_341", "a2l5_342",
  
  # Ã–konomische Faktoren:
  "a3l1_343", "a3l2_344", "a3l3_345", "a3l4_346", "a3l5_347", "a3l6_348",
  
  # Demografische Variablen
  "Age", "FamilySt", "FamilyPrsNr", 
  "FamilyChldNr_SQ002", "FamilyChldNr_SQ003", "FamilyChldNr_SQ004", 
  "FamilyChldNr_SQ005", "FamilyChldNr_SQ006", "FamilyChldNr_SQ007",
  
  # Bildung & Beruf
  "EduSD", "Occup", "Office",
  
  # Big Five PersÃ¶nlichkeit
  "SklBig5_G1", "SklBig5_E1", "SklBig5_O1", "SklBig5_N1", "SklBig5_V1",
  "SklBig5_E2", "SklBig5_N2", "SklBig5_G2", "SklBig5_V2", "SklBig5_O2",
  
  # Einkommen & Ausgaben
  "NetIncome", "NetIncomeFree_ResiRnt", "NetIncomeFree_consumption", 
  "NetIncomeFree_savings",
  
  # Ausgabenkategorien
  "Spendings_NonFood", "Spendings_Sport", "Spendings_BodyCare", 
  "Spendings_Leissure", "Spendings_VocTrng", "Spendings_PrvtTrng",
  "Spendings_EDU", "Spendings_Gastro", "Spendings_Culture"
)

cat("\n Feature-Auswahl mit Include-Liste:\n")
cat("GewÃ¼nschte Features:", length(include_vars), "\n\n")

# PRÃœFUNG 1: Welche Variablen existieren im Datensatz?
existing_vars <- intersect(include_vars, names(data_clean))
missing_vars <- setdiff(include_vars, names(data_clean))

cat("Existierende Features:", length(existing_vars), "\n")

if(length(missing_vars) > 0) {
  cat("\n Fehlende Features (", length(missing_vars), "):\n", sep="")
  print(missing_vars)
  cat("\nDiese werden ignoriert.\n")
}

# PRÃœFUNG 2: Sind alle numerisch?
non_numeric <- existing_vars[!sapply(data_clean[existing_vars], is.numeric)]
if(length(non_numeric) > 0) {
  cat("\n Nicht-numerische Features (", length(non_numeric), "):\n", sep="")
  print(non_numeric)
  cat("Diese werden entfernt.\n")
  existing_vars <- setdiff(existing_vars, non_numeric)
}

# Finalisiere Feature-Liste
feature_cols <- existing_vars
cat("\n Finale Feature-Anzahl:", length(feature_cols), "\n")

# Erstelle Basis-Datensatz (Features + Zielvariable)
data_basic <- data_clean %>%
  select(all_of(c(feature_cols, target_var)))

# PRÃœFUNG 3: Entferne Spalten mit Standardabweichung = 0
zero_sd_cols <- names(which(apply(data_basic[feature_cols], 2, 
                                   function(x) sd(x, na.rm = TRUE)) == 0))
if(length(zero_sd_cols) > 0) {
  cat("\n  Entferne", length(zero_sd_cols), "Features mit SD=0:\n")
  print(zero_sd_cols)
  feature_cols <- setdiff(feature_cols, zero_sd_cols)
  data_basic <- data_basic %>% select(all_of(c(feature_cols, target_var)))
}
```
Verteilung der Zielvariable und Berechnung Korrelationen:
```{r eda}
# ---- EXPLORATIVE DATENANALYSE (EDA) ----
cat("\nðŸ“ˆ Explorative Datenanalyse...\n")

# Verteilung der Zielvariable
p_target <- ggplot(data_basic, aes_string(x = target_var)) +
  geom_histogram(bins = 7, fill = "steelblue", alpha = 0.7, color = "black") +
  geom_density(aes(y = ..count..), alpha = 0.3, fill = "red") +
  labs(title = paste("Verteilung der Zielvariable:", target_var),
       x = "Likert-Skala (1-7)", y = "HÃ¤ufigkeit") +
  theme_minimal() +
  scale_x_continuous(breaks = 1:7)

print(p_target)

# Korrelationsanalyse fÃ¼r Top Features
feature_data <- data_basic %>%
  select(-all_of(target_var))

# Berechne Korrelationen zur Zielvariable
correlations <- cor(feature_data, data_basic[[target_var]], use = "complete.obs")
correlations <- abs(as.vector(correlations))
names(correlations) <- colnames(feature_data) 
correlations <- sort(correlations, decreasing = TRUE)

# Top Features fÃ¼r Korrelationsmatrix
top_features <- names(correlations)[1:min(20, length(correlations))]

# Korrelationsmatrix plotten
cor_matrix <- cor(data_basic[c(top_features, target_var)], use = "complete.obs")
corrplot(cor_matrix, method = "color", type = "upper", 
         tl.cex = 0.8, tl.col = "black", title = "Korrelationsmatrix (Top Features)")
```
Train-Test-Split, Imputation mithilfe des Random Forests: 
```{r initial_rf_training}
# ---- RANDOM FOREST TRAINING (fÃ¼r Feature Importance) ----
cat("\nðŸŒ² Erstes Random Forest Training fÃ¼r Feature Importance...\n")

# Train-Test Split
set.seed(42)
trainIndex <- createDataPartition(data_basic[[target_var]], p = 0.7, list = FALSE)
train_basic <- data_basic[trainIndex, ]
test_basic <- data_basic[-trainIndex, ]

cat("Training:", nrow(train_basic), "Beobachtungen\n")
cat("Test:", nrow(test_basic), "Beobachtungen\n")

# Fehlende Werte schÃ¤tzen und Random Forest trainieren
# rfImpute fÃ¼hrt eine Iteration von Imputation und Modelltraining durch
# Bereinige den Datensatz, um nur die Features und die Zielvariable zu behalten
data_for_imputation <- as.data.frame(train_basic) %>%
  select(all_of(feature_cols), all_of(target_var))
data_for_imputation_test <- as.data.frame(test_basic) %>%
  select(all_of(feature_cols), all_of(target_var))

# PrÃ¼fe Missing Values
missing_count <- sum(is.na(data_for_imputation))
cat("Fehlende Werte im Datensatz:", missing_count, "\n")
missing_count_test <- sum(is.na(data_for_imputation_test))
cat("Fehlende Werte im Test-Datensatz:", missing_count_test, "\n")

  # rfImpute fÃ¼r die Feature-Matrix (ohne Zielvariable)
  X_imputed <- rfImpute(
    x = data_for_imputation[feature_cols],
    y = data_for_imputation[[target_var]],
    iter = 5,
    ntree = 1000
  )
  
  # Kombiniere imputierte Features mit Zielvariable
  data_imputed <- data.frame(
    X_imputed,
    target = data_for_imputation[[target_var]]
  )
  names(data_imputed)[ncol(data_imputed)] <- target_var

# FÃ¼r den Test-Datensatz:
  
 # rfImpute fÃ¼r die Feature-Matrix (ohne Zielvariable)
  X_imputed_test <- rfImpute(
    x = data_for_imputation_test[feature_cols],
    y = data_for_imputation_test[[target_var]],
    iter = 5,
    ntree = 1000
  )
  
  # Kombiniere imputierte Features mit Zielvariable
  data_imputed_test <- data.frame(
    X_imputed_test,
    target = data_for_imputation_test[[target_var]]
  )
  names(data_imputed_test)[ncol(data_imputed_test)] <- target_var
  
  # Jetzt Random Forest auf imputierten Daten trainieren
  rf_imputed_test <- randomForest(
    x = data_imputed_test[feature_cols],
    y = data_imputed_test[[target_var]],
    ntree = 1000,
    importance = TRUE
  )

cat("âœ… Imputation und Random Forest Modell erfolgreich trainiert.\n")

```

Training des Random Forests auf den imputierten Daten inkl. Hyperparameteroptimierung:
```{r enhanced_rf_training}
# ---- RANDOM FOREST MIT HYPERPARAMETER TUNING ----
cat("\n RANDOM FOREST TRAINING\n")

# Hyperparameter Grid
param_grid <- expand.grid(
  mtry = c(
    floor(sqrt(length(feature_cols))),
    floor(length(feature_cols)/3),
    floor(length(feature_cols)/2),
    floor(length(feature_cols) * 0.8)
  ),
  splitrule = c("variance", "extratrees"),
  min.node.size = c(5, 10, 20)
)

# Cross-Validation Setup
ctrl <- trainControl(
  method = "cv",
  number = 5,
  verboseIter = TRUE,
  allowParallel = TRUE
)

cat("Starte Hyperparameter-Tuning mit", nrow(param_grid), "Kombinationen...\n")
cat("Features verwendet:", length(feature_cols), "\n")

# Training mit erweiterten Features
rf_enhanced <- train(
  x = data_imputed[feature_cols], #enhanced_train
  y = data_imputed[[target_var]],
  method = "ranger",
  trControl = ctrl,
  tuneGrid = param_grid,
  num.trees = 1000,
  importance = "impurity",
  respect.unordered.factors = "order"
)

cat("âœ… Enhanced Random Forest trainiert!\n")
print(rf_enhanced$bestTune)

# Feature Importance aus enhanced model
enhanced_importance <- rf_enhanced$finalModel$variable.importance
enhanced_importance_df <- data.frame(
  Variable = names(enhanced_importance),
  Importance = as.numeric(enhanced_importance)
) %>%
  arrange(desc(Importance)) %>%
  head(20)

print(enhanced_importance_df)
```

Berechnung der GÃ¼temaÃŸe des Modells:
```{r model_evaluation}
# ---- 10. COMPREHENSIVE MODEL EVALUATION ----
cat("\nðŸ“Š MODELL-EVALUATION\n")

# Vorhersagen
#basic_pred <- predict(rf_imputed, data_imputed_test[feature_cols])
enhanced_pred <- predict(rf_enhanced, data_imputed_test[feature_cols])

# Metriken berechnen
enhanced_mae <- mae(data_imputed_test[[target_var]], enhanced_pred)
enhanced_rmse <- rmse(data_imputed_test[[target_var]], enhanced_pred)

ss_res <- sum((test_basic[[target_var]] - enhanced_pred)^2)
ss_tot <- sum((test_basic[[target_var]] - mean(test_basic[[target_var]]))^2)
enhanced_r2 <- 1 - (ss_res / ss_tot)

# Baseline (Mittelwert)
baseline_pred <- mean(data_imputed[[target_var]])
baseline_mae <- mae(data_imputed_test[[target_var]], rep(baseline_pred, nrow(data_imputed_test)))
baseline_rmse <- rmse(data_imputed_test[[target_var]], rep(baseline_pred, nrow(data_imputed_test)))

# Ergebnisse
cat("ðŸŽ¯ MODELL-VERGLEICH:\n")
cat("Enhanced  MAE:", round(enhanced_mae, 4), "| RMSE:", round(enhanced_rmse, 4), "| RÂ²:", round(enhanced_r2, 4), "\n")

# Visualisierungen
plot_data <- data.frame(
  Actual = data_imputed_test[[target_var]],
  #Basic_Pred = basic_pred,
  Enhanced_Pred = enhanced_pred
)
y_test <- data_imputed_test[[target_var]]

rf_ergebnis <- ggplot(data.frame(Actual = y_test, Predicted = enhanced_pred), 
                      aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.5, color = "steelblue") +
  geom_abline(intercept = 0, slope = 1, color = "red", linewidth = 1) +
  xlim(1, 7) +
  ylim(1, 7) +
  coord_fixed(ratio = 1) +  # Gleiche Skalierung fÃ¼r x und y
  labs(x = "TatsÃ¤chliche Werte",
       y = "Vorhergesagte Werte",
       title = "Random Forest: Regression") +
  theme_minimal()

print(rf_ergebnis)

ggsave(filename = paste0("~/Masterthesis/plots/RF/Regression", 
                     "_", target_var, ".png"), 
       plot = rf_ergebnis, 
       width = 8, height = 6, dpi = 300, bg = "white")

```

OLS berechnen und bewerten:
```{r}
# Berechne OLS-Regression
top_features_ols <- enhanced_importance_df$Variable[nrow(enhanced_importance_df)]

# Erstelle Formel fÃ¼r OLS
ols_formula <- reformulate(top_features_ols, target_var)

# Trainiere OLS auf den imputierten Daten
ols_data <- data_imputed[c(top_features_ols, target_var)]
ols_model <- lm(ols_formula, data = ols_data[trainIndex, ])

# OLS Vorhersagen
ols_pred <- predict(ols_model, newdata = data_imputed_test[top_features_ols])

# OLS Metriken
ols_mae <- mae(data_imputed_test[[target_var]], ols_pred)
ols_rmse <- rmse(data_imputed_test[[target_var]], ols_pred)
ols_r2 <- summary(ols_model)$r.squared

cat("OLS Performance:\n")
cat("R2 von OLS:", ols_r2, "\n")
cat("RMSE:", ols_rmse, "\n")

# OLS Koeffizienten-Analyse
ols_coef <- summary(ols_model)$coefficients
significant_coef <- ols_coef[ols_coef[, 4] < 0.05, ]

cat("\nSignifikante OLS-Koeffizienten (p < 0.05):\n")
print(round(significant_coef, 4))

ols_ergebnis <- ggplot(data.frame(Actual = y_test, Predicted = ols_pred), 
                      aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.5, color = "steelblue") +
  geom_abline(intercept = 0, slope = 1, color = "red", linewidth = 1) +
  xlim(1, 7) +
  ylim(1, 7) +
  coord_fixed(ratio = 1) +  
  labs(x = "TatsÃ¤chliche Werte",
       y = "Vorhergesagte Werte",
       title = "OLS: Regression") +
  theme_minimal()

print(ols_ergebnis)

ggsave(filename = paste0("~/Masterthesis/plots/OLS/Regression", 
                     "_", target_var, ".png"), 
       plot = ols_ergebnis, 
       width = 8, height = 6, dpi = 300, bg = "white")
```

Quellenangaben der verwendeten Packages:
```{r}
citation()
cite_packages()
```

